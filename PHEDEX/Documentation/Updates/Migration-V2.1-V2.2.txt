2.1 -> 2.2 MIGRATION LOG
------------------------

Steps in this migration are as follows
1. Make a copy of the Dev database on the Testbed
2. Establish migration process on Testbed
3. Test agents against Testbed
4. Drop old Testbed schema
5. Migrate Dev (should be smooth, scripted)
6. Create roles, synonyms on Dev
7. Test agents against Dev
8. Drop old Dev schema
9. Migrate Prod (should be flawless)

As defined in 2.2 DBParam files
Testbed => devdb testbed 
Dev => devdb proper
Prod => cms


1. Making a testbed copy of Dev on the Testbed
----------------------------------------------
1.0 Pre-copy preparation
1.1 Drop all tables in the Testbed
1.2 Create a 2.1 schema on Testbed
1.3 Copy all tables and contents from devdb to Testbed

1.0 Pre-copy preparation
------------------------
We need to set up some environment variables for using this
migration log effectively. I did this on lcgui02.gridpp.rl.ac.uk

# export TESTBEDUSER=XXX
# export TESTBEDPASS=XXX
# export DEVUSER=XXX
# export DEVPASS=XXX
export BASE=/data/tim/MigrationNodes
export CVSROOT=:pserver:anonymous@cmscvs.cern.ch:/cvs_server/repositories/PHEDEX
cvs login
mkdir $BASE
cd $BASE


1.1 Drop all tables in the Testbed
----------------------------------



PLEASE NOTE!!!!!
DROPPING THE TABLES IS *NOT* TYPICALLY THE FIRST THING YOU DO DURING A
MIGRATION! THIS IS A SPECIAL STEP TO GIVE US SPACE ON THE TESTBED. THINK
BEFORE DOING!



cat > ./generate-drop.sql << "EOF"
set pages 0;
set feedback off;
select 'drop table ' || table_name || ' cascade constraints;' from user_tables;
exit;
EOF

echo "set pages 0;" > drop-tables.sql
sqlplus -S $TESTBEDUSER/$TESTBEDPASS@devdb @generate-drop.sql >> drop-tables.sql
echo "exit;" >> drop-tables.sql

sqlplus $TESTBEDUSER/$TESTBEDPASS@devdb @drop-tables.sql

    PROBLEM: If you happen to loop back and have to do this again,
    you'll need to delete triggers and sequences.
    
    SOLUTION:
    cat > drop-triggers.sql << "EOF"
    drop trigger new_transfer_state;   
    drop trigger update_transfer_state;
    drop sequence seq_dbs_file;
    drop sequence seq_dbs_run;
    drop sequence seq_dbs_block;
    drop sequence seq_dbs_dataset;
    drop sequence seq_request_id;
    EOF

    FIXME: Can incorporate trigger dropping etc in generate-drop.sql- just
    add
    
    select 'drop trigger ' || trigger_name || ';' from user_triggers;
    select 'drop sequence ' || sequence_name || ';' from user_sequences;


1.2 Create a V2.1 schema on Testbed
-----------------------------------
Pick the latest tag of the version you're interested in

cvs co -r PHEDEX_V2_1_20050409 PHEDEX/Schema
cd PHEDEX/Schema

Modify the SQL to use devdb tablespaces!
perl -p -i -e 's/([ ])CMS_TRANSFERMGMT_INDX01/${1}INDX01/g' *.sql

sqlplus ${TESTBEDUSER}/${TESTBEDPASS}@devdb < OracleInit.sql


1.3 Copy all tables and contents from devdb to Testbed
------------------------------------------------------
THIS BIT REQUIRES THOUGHT
At various stages in this step you'll need to modify the scripts by
hand. You'll need to think about the order in which you insert data
into the Testbed TMDB so that you satisfy integrity constraints.
You may also find that there are differences between the published
schema and the current Dev schema; you need to bring the Testbed
schema into line with what's in Dev in order to practice the
Dev migration properly.

cat > ./generate-copy.sql << "EOF"
-- Insert correct usernames here by hand!
set pages 0;
set feedback off;
set linesize 1000;
select 'copy from $DEVUSER/$DEVPASS@devdb to $TESTBEDUSER/$TESTBEDPASS@devdb'
    || ' insert '
    || table_name
    || ' using select * from '
    || table_name
    || ' ;' from user_tables;
exit;
EOF
emacs -nw generate-copy.sql

echo "-- Modify this so that things are in constraint-satisfying order!" > copy-data.sql
echo "set pages 0;" >> copy-data.sql
echo "set feedback off;" >> copy-data.sql
echo "set linesize 1000;" >> copy-data.sql
sqlplus -S $DEVUSER/$DEVPASS@devdb @generate-copy.sql >> copy-data.sql
echo "exit;" >> copy-data.sql
emacs -nw copy-data.sql

For this step I had, after modification:

cat copy-data.sql 
-------------------------------------------
set pages 0;
set feedback off;
set linesize 1000;

copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_NODE using select * from T_NODE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_NODE_EXPORT using select * from T_NODE_EXPORT ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_NODE_IMPORT using select * from T_NODE_IMPORT ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_FILE using select * from T_FILE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_FILE_ATTRIBUTES using select * from T_FILE_ATTRIBUTES ;

copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_AGENT using select * from T_AGENT ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_AGENT_MESSAGE using select * from T_AGENT_MESSAGE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_AGENT_STATUS using select * from T_AGENT_STATUS ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_BLOCK using select * from T_BLOCK ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_BLOCK_REPLICA using select * from T_BLOCK_REPLICA ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_DESTINATION using select * from T_DESTINATION ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_AGENT_STATUS using select * from T_INFO_AGENT_STATUS ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_FILE_SIZE_HISTOGRAM using select * from T_INFO_FILE_SIZE_HISTOGRAM ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_FILE_SIZE_OVERVIEW using select * from T_INFO_FILE_SIZE_OVERVIEW ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_REPLICATION_DETAILS using select * from T_INFO_REPLICATION_DETAILS ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_REPLICATION_OVERVIEW using select * from T_INFO_REPLICATION_OVERVIEW ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_SUBSCRIPTIONS using select * from T_INFO_SUBSCRIPTIONS ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_TRANSFER_RATE using select * from T_INFO_TRANSFER_RATE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_INFO_TRANSFER_STATUS using select * from T_INFO_TRANSFER_STATUS ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_REPLICA_STATE using select * from T_REPLICA_STATE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_ROUTING using select * from T_ROUTING ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_SUBSCRIPTION using select * from T_SUBSCRIPTION ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_TRANSFER_COMPLETED using select * from T_TRANSFER_COMPLETED ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_TRANSFER_HISTORY using select * from T_TRANSFER_HISTORY ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_TRANSFER_STATE using select * from T_TRANSFER_STATE ;
copy from cms_transfermgmt/smalland_round@devdb to cms_transfermgmt_testbed/phedexedehp@devdb insert T_TRANSFER_SUMMARY using select * from T_TRANSFER_SUMMARY ;
exit;
----------------------------------------


Then to copy the data:

sqlplus $TESTBEDUSER/$TESTBEDPASS@devdb @copy-data.sql


2. Establish migration process on Testbed
-----------------------------------------

2.0 Prep
2.1 Save existing schema
2.2 Load new schema
2.3 Map old data onto new schema

2.0 Preparation
---------------
A recap from 1.0

# export TESTBEDUSER=XXX
# export TESTBEDPASS=XXX
# export DEVUSER=XXX
# export DEVPASS=XXX
export BASE=/data/tim/MigrationNode
export CVSROOT=:pserver:anonymous@cmscvs.cern.ch:/cvs_server/repositories/PHEDEX
cvs login
mkdir -p $BASE
cd $BASE

    FIXME: Also need to have checked out PHEDEX/Schema from CVS here!
    
    FIXME: Also assumes you have all the tools installed (POOL, OIC, perl)!

2.1 Save existing schema
------------------------
cd $BASE/PHEDEX/Schema
./OracleSave.sh $TESTBEDUSER/$TESTBEDPASS@devdb

    PROBLEM: Couldn't rename IX_TRANSFER_STATE_FROMTO_STATE because 
    the new name was too long. Oracle identifiers longer than 30 chars 
    aren't possible!

    SOLUTION: By hand I renamed this Index XIX_FROMTO_STATE. 
    
    A look at Dev shows that this index has been removed already.


2.2 Load new schema
-------------------
Choose the latest CVS release tag (e.g. for 2.2)

cd $BASE
rm -fr PHEDEX
cvs co -r PHEDEX_2_2_20050601 PHEDEX/Schema

    PROBLEM: Originally I tried to just cvs update to the latest 
    Schema, but found conflicts during a merge.

    SOLUTION: Just deleted the repo and checked out as described above.

cd PHEDEX/Schema

Modify the SQL to use devdb tablespaces!
perl -p -i -e 's/([ ])CMS_TRANSFERMGMT_INDX01/${1}INDX01/g' *.sql

sqlplus ${TESTBEDUSER}/${TESTBEDPASS}@devdb < OracleInit.sql

    PROBLEM: Sequences reported as already existing. This is because
    I've already used OracleInit.sh to create a 2.1 schema on Testbed.
    My drop script above does not drop sequences.
    
    SOLUTION: Use drop-trigger.sql described in 1.1. Or improved version
    of generate-drop.sql, better.
    
2.3 Map old data onto new schema
--------------------------------
This will clearly vary from migration to migration, and requires a
fair amount of thought! Best to do by hand on SQLplus command line.

    FIXME: Groupings should follow PHEDEX/Schema/Oracle*.sql
    groupings. Order is typically node-related; file-related; other.

cat > migrate.sql << "EOF"
insert into t_node
    (select name from xt_node);
insert into t_file 
    (select timestamp,guid,node,inblock,insubblock,
        lfn,filetype,filesize,checksum
    from xt_file);
insert into t_file_attributes
    (select guid,attribute,value from xt_file_attributes);
insert into t_node_import
    (select node,protocol,priority from xt_node_import);
insert into t_node_export
    (select node,protocol from xt_node_export);
insert into t_node_neighbour 
    (select from_node,to_node,hops
        from xt_routing
        where to_node = gateway);
insert into t_replica_state
    (select timestamp,guid,node,state,state_timestamp 
        from xt_replica_state);
alter trigger new_transfer_state disable;
alter trigger update_transfer_state disable;
insert into t_transfer_state
    (select timestamp,guid,to_node,to_state,to_timestamp,
            from_node,from_state,from_timestamp,from_pfn
        from xt_transfer_state);
alter trigger new_transfer_state enable;
alter trigger update_transfer_state enable;
insert into t_transfer_completed
    (select timestamp,guid,to_node,to_state,to_timestamp,
            from_node,from_state,from_timestamp
        from xt_transfer_completed);    
insert into t_transfer_history
    (select timestamp,guid,to_node,to_old_state,to_new_state,
            from_node,from_old_state,from_new_state
        from xt_transfer_history);
insert into t_transfer_summary
    (select timestamp,guid,from_node,to_node,assigned,wanted1st,wanted,
            exported,started,completed,cleared,errors,error_total,error_began
        from xt_transfer_summary);
insert into t_subscription
    (select owner,dataset,destination from xt_subscription);
insert into t_block 
    (select 0, name, owner, dataset, 0, 0, 1 
        from xt_block);
update t_block 
    set (timestamp, files, bytes) = 
        (select min(timestamp), count(guid), sum(filesize) 
            from t_file 
            where inblock = name);
EOF

To migrate, use

sqlplus $TESTBEDUSER/$TESTBEDPASS@devdb @migrate.sql

There's no commits- you may want to commit manually at various stages, 
or be able to rollback at will. I'd definitely recommend doing each
step by hand the first few times you try it.

    PROBLEM: ORA-01536: space quota exceeded for tablespace 'INDX01'
    
    SOLUTION: Archive contents of t_transfer_history and delete from
    table.
    
    cvs co PHEDEX/Utilities
    cvs co PHEDEX/Toolkit
    export PATH=$PATH:$BASE/PHEDEX/Utilities
    export DBPARAM=/some/DBparam/file
    
    DBDump -db /home/csf/phtab/PhEDExDev/DBParam:Testbed    \
        xt_transfer_history                                 \ 
        | gzip > xt_transfer_history_20050606.gz
        
    Now truncate the table to make space
    truncate table xt_transfer_history drop storage;
    
    
    PROBLEM: There are more rows in Testbed xt_transfer_history than
    in Dev t_transfer history! (282185/269912). Was there a problem with
    the copy process?
    
    cat xt_transfer_history_20050606 | sort | uniq | wc -l
    suggests there are no repeated entries in Testbed!
     
    SOLUTION: No idea at the moment. Have left, have archive of table.
    Watch out for this when copying next time!
    
    
    
3. Test agents against Testbed
------------------------------
Here we want to run agents on a single machine that pretend that they
comprise the real Dev PhEDEx infrastructure.

I'm running these tests as barrass on lxcmsf1.

3.0 General environment preparation
3.1 Generate fake publish and pfn export scripts
3.2 Generate Config files and test agents

3.0 General environment preparation
-----------------------------------

Note that when checking out PhEDEx from CVS you should tag a version that
corresponds to the head. Tags below are included as examples only, not
prescriptions.

I did this on lxcmsf1.

export BASE=/data/tim/MigrationNodes
mkdir $BASE
cd $BASE
mkdir Tools 
# Copied the DBParam file to $BASE
# Copied the Oracle Instant Client zips to $BASE/Tools
export CVSROOT=:pserver:anonymous@cmscvs.cern.ch:/cvs_server/repositories/PHEDEX
cvs login
cvs co -r PHEDEX_2_2_20050601 PHEDEX/Deployment
cvs co -r PHEDEX_2_2_20050601 PHEDEX/Toolkit
cvs co -r PHEDEX_2_2_20050601 PHEDEX/Utilities
cvs co -r PHEDEX_2_2_20050601 PHEDEX/Schema
export PATH=$PATH:$BASE/PHEDEX/Deployment:$BASE/PHEDEX/Utilities 

cd Tools
VO_CMS_SW_DIR=/afs/cern.ch/sw InstallPOOL -arch SLC3 -cms $BASE/Tools
. ./poolenv.sh
InstallOracleClient $BASE/Tools $BASE/Tools $BASE/PHEDEX/Schema
. ./oraenv.sh
InstallPerlModules $BASE/Tools
. ./perlenv.sh

    PROBLEM: Setting environment when you log back in.
    
    SOLUTION: In 3.2 we create Common.config: just
    . ./Common.config

3.1 Generate fake publish and pfn export scripts
------------------------------------------------

    FIXME: Would be good to store these in PHEDEX/Testbed/Something.
    Suggest /Migration, rework this section to use the test framework,
    so that basic log checking is easier!

cat > Transfer_URLmapper.sh << "EOF"
#!/bin/sh

node= dataset= owner= lfn=
for arg; do
  case $arg in
    node=*) node=$(echo $arg | sed 's![^=]*=!!') ;;
    guid=*) guid=$(echo $arg | sed 's![^=]*=!!') ;;
    owner=*) owner=$(echo $arg | sed 's![^=]*=!!') ;;
    dataset=*) dataset=$(echo $arg | sed 's![^=]*=!!') ;;
    lfn=*) lfn=$(echo $arg | sed 's![^=]*=!!') ;;
  esac
done

case $owner in
    *Hit*) subpath='Hit' ;;
    *PU*) subpath='Digi' ;;
    *DST*) subpath='DSTs';;
esac

case $lfn in
    *.ntuple) subpath='CMKin';;
    *.fz) subpath='CMSim';;
esac

case $dataset in
    *MBforPU*) subpath='MBforPU' ;;
esac

[ -z "$subpath" ] && { echo "no subpath" 1>&2; exit 1; }
[ -z "$owner" ] && { echo "no owner" 1>&2; exit 1; }
[ -z "$dataset" ] && { echo "no dataset" 1>&2; exit 1; }
[ -z "$lfn" ] && { echo "no lfn" 1>&2; exit 1; }
 
# Copy everything into a subdirectories of a local directory.
basedir="${node}"
local=$basedir/$subpath/$dataset/$lfn
echo file:/$local |perl -ne 'print length() < 250 ? $_ : substr($_, 0, 210)."'$guid'"'
EOF

cat > Publish.pl << "EOF"
#!/usr/bin/env perl

use strict;
use warnings;

my $pool_cat = $ARGV[0];
my $node = $ARGV[1];
my $guid = $ARGV[2];
my $pfn = $ARGV[3];
my $xml = $ARGV[4];

# check, that we got all argument we need
if (scalar(@ARGV) != 5) {
    print "not all arguments for publish script set.\n";
    print "argument 1: POOL contact string for local catalogue\n";
    print "argument 2: node suffix\n";
    print "argument 3: GUID to publish\n";
    print "argument 4: localized PFN\n";
    print "argument 5: Path to xml pool fragment\n";
    exit 5;
}

# change PFN to local access
my $locpfn = "$pfn"."_"."$node";


#check if we already have a copy of the GUID
#in this case just add the replica to the POOL catalogue
if (1) {
#if (`FClistPFN -u $pool_cat -q "guid='$guid'" |grep -v Info |grep -v Debug |grep -v Warning`) {
#    my $cmd="FCaddReplica -u $pool_cat -r $locpfn -g $guid >& /dev/null";
    my $cmd="/bin/true";
    my $err = system($cmd);
    if ($err) {
        print "couldn't add local PFN to POOL catalogue (exit 1)\n";
        exit 1;
    }
} else {
# if we don't know the GUID at all, we have to publish all information
# to the POOL catalogue (metadata and stuff)
    
# add the local PFN to the catalogue fragment
    my $cmd="FCaddReplica -u xmlcatalog_file:$xml -r $locpfn -g $guid";
    my $err = system($cmd);
    if ($err) {
        print "couldn't add local PFN to catalogue fragment (exit 2)\n";
        exit 2;
    }
# delete old PFN from catalogue fragment
    $cmd="FCdeletePFN -u xmlcatalog_file:$xml -p $pfn";
    $err = system($cmd);
    if ($err) {
        print "couldn't delete PFN from catalogue fragment (exit 3)\n";
        exit 3;
    }
# publish the catalogue fragment too POOL
#    $cmd="FCpublish -u xmlcatalog_file:$xml -d $pool_cat >& /dev/null";
    $cmd='/bin/true';
    $err = system($cmd);
    if ($err) {
        print "couldn't publish fragment to POOL (exit 4)\n";
        exit 4;
    }
}
exit 0;
EOF

cat > PFNLookup.sh << "EOF"
#!/bin/sh

##H Usage: SC2Lookup -g PROTO FOR-NODE GUID...
##H
##H List GUID/PFN pairs for GUIDs (only -g GUID is supported).
##H
##H This fakes out a catalogue for SC2 transfer challenge from
##H CERN using PhEDEx, returning one of the 40 predetermined
##H test files from SC2 server.
##H
##H The only supported protocol is "srm".

# Pick up options
cat= match= mode= proto= fornode= rewrite=
while [ $# -ge 1 ]; do
  case $1 in
    -g )
      mode=$1; shift ;;
    -h )
      grep "^##H" < $0 | sed 's/^##H\( \|\)//'; exit 1 ;;
    -u )
      shift; shift ;;
    -n )
      shift; suffix=$1; shift ;;
    -* )
      echo "unrecognised option $1" 1>&2; exit 1 ;;
    * )
      break ;;
  esac
done

proto="$1"; shift
fornode="$1"; shift

[ X"$mode" = X ] && { echo "$0: no lookup mode specified" 1>&2; exit 1; }
[ X"$mode" != X-g ] && { echo "$0: lookup mode $mode not supported, '-g' required" 1>&2; exit 1; }
[ X"$proto" = X ] && { echo "$0: no protocol specified" 1>&2; exit 1; }
[ X"$fornode" = X ] && { echo "$0: no destination node specified" 1>&2; exit 1; }

lfnbase=$proto://radiantservice.cern.ch:8443/castor/cern.ch/grid/dteam/storage/transfer-test/sample-file-1gig.$suffix
for guid; do
  case $guid in
    ??????[0A][0123]-*) echo $guid $lfnbase-00.dat ;;
    ??????[0A][4567]-*) echo $guid $lfnbase-01.dat ;;
    ??????[0A][89AB]-*) echo $guid $lfnbase-02.dat ;;
    ??????[0A][CDEF]-*) echo $guid $lfnbase-03.dat ;;

    ??????[1B][0123]-*) echo $guid $lfnbase-04.dat ;;
    ??????[1B][4567]-*) echo $guid $lfnbase-05.dat ;;
    ??????[1B][89AB]-*) echo $guid $lfnbase-06.dat ;;
    ??????[1B][CDEF]-*) echo $guid $lfnbase-07.dat ;;

    ??????[2C][0123]-*) echo $guid $lfnbase-08.dat ;;
    ??????[2C][4567]-*) echo $guid $lfnbase-09.dat ;;
    ??????[2C][89AB]-*) echo $guid $lfnbase-10.dat ;;
    ??????[2C][CDEF]-*) echo $guid $lfnbase-11.dat ;;

    ??????[3D][0123]-*) echo $guid $lfnbase-12.dat ;;
    ??????[3D][4567]-*) echo $guid $lfnbase-13.dat ;;
    ??????[3D][89AB]-*) echo $guid $lfnbase-14.dat ;;
    ??????[3D][CDEF]-*) echo $guid $lfnbase-15.dat ;;

    ??????[4E][0123]-*) echo $guid $lfnbase-16.dat ;;
    ??????[4E][4567]-*) echo $guid $lfnbase-17.dat ;;
    ??????[4E][89AB]-*) echo $guid $lfnbase-18.dat ;;
    ??????[4E][CDEF]-*) echo $guid $lfnbase-19.dat ;;

    ??????[5F][0123]-*) echo $guid $lfnbase-20.dat ;;
    ??????[5F][4567]-*) echo $guid $lfnbase-21.dat ;;
    ??????[5F][89AB]-*) echo $guid $lfnbase-22.dat ;;
    ??????[5F][CDEF]-*) echo $guid $lfnbase-23.dat ;;

    ??????[66][0123]-*) echo $guid $lfnbase-24.dat ;;
    ??????[66][4567]-*) echo $guid $lfnbase-25.dat ;;
    ??????[66][89AB]-*) echo $guid $lfnbase-26.dat ;;
    ??????[66][CDEF]-*) echo $guid $lfnbase-27.dat ;;

    ??????[77][0123]-*) echo $guid $lfnbase-28.dat ;;
    ??????[77][4567]-*) echo $guid $lfnbase-29.dat ;;
    ??????[77][89AB]-*) echo $guid $lfnbase-30.dat ;;
    ??????[77][CDEF]-*) echo $guid $lfnbase-31.dat ;;

    ??????[88][0123]-*) echo $guid $lfnbase-32.dat ;;
    ??????[88][4567]-*) echo $guid $lfnbase-33.dat ;;
    ??????[88][89AB]-*) echo $guid $lfnbase-34.dat ;;
    ??????[88][CDEF]-*) echo $guid $lfnbase-35.dat ;;

    ??????[99][0123]-*) echo $guid $lfnbase-36.dat ;;
    ??????[99][4567]-*) echo $guid $lfnbase-37.dat ;;
    ??????[99][89AB]-*) echo $guid $lfnbase-38.dat ;;
    ??????[99][CDEF]-*) echo $guid $lfnbase-39.dat ;;
  esac
done
EOF

3.2 Generate Config files and test agents
-----------------------------------------
First config script sets up a general environment

cat > Common.config << "EOF"
### ENVIRON common
export BASE=/data/tim/MigrationNodes;
export PATH=$PATH:$BASE/PHEDEX/Deployment:$BASE/PHEDEX/Utilities;
export DBPARAM=$BASE/DBParam;
export TMDB=Testbed;
export POOL_OUTMSG_LEVEL=100;
export PHEDEX_SCRIPTS=$BASE/PHEDEX;
export PHEDEX_CUSTOM=$BASE;
export PHEDEX_MYSQL=mysqlcatalog_mysql://phedex:phedex@cmslcgco04/phedexcat;
export PHEDEX_CATALOGUE=${PHEDEX_MYSQL};
. $BASE/Tools/poolenv.sh;
. $BASE/Tools/oraenv.sh;
. $BASE/Tools/perlenv.sh;
EOF

Now a script to set up CERN specific nodes: I recommend starting the agents
up after this in distint groups: routers, blocks, monitoring, transfer...

cat > CERN.config << "EOF"
### ENVIRON CERN
PHEDEX_SITE=T1_CERN;
PHEDEX_LOGS=$BASE/$PHEDEX_SITE/logs;
PHEDEX_STATE=$BASE/$PHEDEX_SITE/incoming;

#### Export-side agents
### AGENT LABEL=cern-exp-pfn PROGRAM=Toolkit/Transfer/FilePFNExport ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -pfnquery      ${PHEDEX_CUSTOM}/PFNLookup.sh,-u,${PHEDEX_CATALOGUE},-n,castor
 -wait          $(expr 10 + $RANDOM % 10)

### AGENT LABEL=cern-exp-disk PROGRAM=Toolkit/Transfer/FileDiskExport ENVIRON=CERN
 -db            ${DBPARAM}:${TMDB}
 -node          ${PHEDEX_SITE}_Buffer
 -wait          $(expr 50 + $RANDOM % 20)

### AGENT LABEL=cern-exp-upload PROGRAM=Toolkit/Transfer/FileMSSPublish ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -mssnode       ${PHEDEX_SITE}_MSS
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 50 + $RANDOM "%" 20)
 
#### Management agents
### AGENT LABEL=cern-mgmt-rtab PROGRAM=Toolkit/Infrastructure/NodeRouter ENVIRON=CERN
 -nodes         ${PHEDEX_SITE}_Buffer,${PHEDEX_SITE}_MSS
 -db            ${DBPARAM}:${TMDB}
 -timeout       240
 -wait          $(expr 50 + $RANDOM % 20)

### AGENT LABEL=cern-mgmt-router PROGRAM=Toolkit/Infrastructure/FileRouter ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -nodes         ${PHEDEX_SITE}_%
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 50 + $RANDOM % 20)

#### Start monitoring
### AGENT LABEL=cern-info-ds PROGRAM=Toolkit/Monitoring/InfoDropStatus ENVIRON=CERN
 -site          ${PHEDEX_SITE}
 -dir           ${PHEDEX_STATE}
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 60 + $RANDOM % 60)

### AGENT LABEL=cern-info-tc PROGRAM=Toolkit/Monitoring/TableCleaner ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 50 + $RANDOM "%" 20)

### AGENT LABEL=cern-info-ts PROGRAM=Toolkit/Monitoring/InfoTransferStatus ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 150 + $RANDOM "%" 30)

### AGENT LABEL=cern-info-rt PROGRAM=Toolkit/Monitoring/InfoReplicaStates ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 120 + $RANDOM "%" 30)

### AGENT LABEL=cern-info-tt PROGRAM=Toolkit/Monitoring/InfoTransferStates ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 120 + $RANDOM "%" 30)

### AGENT LABEL=cern-info-tr PROGRAM=Toolkit/Monitoring/InfoTransferRate ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 550 + $RANDOM "%" 100)

### AGENT LABEL=cern-info-fs PROGRAM=Toolkit/Monitoring/InfoFileSize ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 10000 + $RANDOM "%" 2000)

### AGENT LABEL=cern-info-sub PROGRAM=Toolkit/Monitoring/InfoSubscriptions ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 10000 + $RANDOM "%" 2000)

### AGENT LABEL=cern-info-rs PROGRAM=Toolkit/Monitoring/InfoReplicationStatus ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 10000 + $RANDOM "%" 2000)

### AGENT LABEL=cern-info-pm PROGRAM=Toolkit/Monitoring/PerfMonitor ENVIRON=CERN
 -db        ${DBPARAM}:${TMDB}
 -wait      $(expr 120 + $RANDOM "%" 30)

#### Block agents
### AGENT LABEL=cern-mgmt-alloc PROGRAM=Toolkit/Workflow/BlockAllocator ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 120 + $RANDOM "%" 30)

### AGENT LABEL=cern-mgmt-blockmon PROGRAM=Toolkit/Workflow/BlockMonitor ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 120 + $RANDOM "%" 30)

### AGENT LABEL=cern-mgmt-blockact PROGRAM=Toolkit/Workflow/BlockActivate ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 600 + $RANDOM "%" 100)

### AGENT LABEL=cern-mgmt-blockdeact PROGRAM=Toolkit/Workflow/BlockDeactivate ENVIRON=CERN
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -holdoff       36000
 -wait          $(expr 600 + $RANDOM "%" 100)

#### Download agents
### AGENT LABEL=cern-download PROGRAM=Toolkit/Transfer/FileDownload DEFAULT=off ENVIRON=CERN
 -node ${PHEDEX_SITE}_Buffer
 -db ${DBPARAM}:${TMDB}
 -backend Globus
 -command /bin/true
 -pfndest ${PHEDEX_CUSTOM}/Transfer_URLmapper.sh,node=${PHEDEX_SITE}
 -publish ${PHEDEX_CUSTOM}/Publish.pl,${PHEDEX_CATALOGUE},${PHEDEX_SITE}
 -timeout 600
 -wanted 200G
 -jobs 1
 -wait $(expr 10 + $RANDOM % 10)

### AGENT LABEL=cern-exp-clean PROGRAM=Toolkit/Transfer/FileFakeCleaner ENVIRON=CERN
 -nodes         ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -backoff       36000
 -wait          $(expr 4 \* 3600 + $RANDOM "%" 1800)

EOF

And another site:

cat > RAL.config << "EOF"
### ENVIRON RAL
PHEDEX_SITE=T1_RAL;
PHEDEX_LOGS=$BASE/$PHEDEX_SITE/logs;
PHEDEX_STATE=$BASE/$PHEDEX_SITE/incoming;

#### Export-side agents
### AGENT LABEL=ral-exp-pfn PROGRAM=Toolkit/Transfer/FilePFNExport ENVIRON=RAL
 -node          ${PHEDEX_SITE}_Buffer
 -db            ${DBPARAM}:${TMDB}
 -pfnquery      ${PHEDEX_CUSTOM}/PFNLookup.sh,-u,${PHEDEX_CATALOGUE},-n,castor
 -wait          $(expr 10 + $RANDOM % 10)

### AGENT LABEL=ral-exp-disk PROGRAM=Toolkit/Transfer/FileDiskExport ENVIRON=RAL
 -db            ${DBPARAM}:${TMDB}
 -node          ${PHEDEX_SITE}_Buffer
 -wait          $(expr 50 + $RANDOM % 20)

#### Management agents
### AGENT LABEL=ral-mgmt-rtab PROGRAM=Toolkit/Infrastructure/NodeRouter ENVIRON=RAL
 -nodes         ${PHEDEX_SITE}_Buffer,${PHEDEX_SITE}_MSS
 -db            ${DBPARAM}:${TMDB}
 -timeout       240
 -wait          $(expr 50 + $RANDOM % 20)

### AGENT LABEL=ral-mgmt-router PROGRAM=Toolkit/Infrastructure/FileRouter ENVIRON=RAL
 -node          ${PHEDEX_SITE}_Buffer
 -nodes         ${PHEDEX_SITE}_%
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 50 + $RANDOM % 20)

#### Start monitoring
### AGENT LABEL=ral-info-ds PROGRAM=Toolkit/Monitoring/InfoDropStatus ENVIRON=RAL
 -site          ${PHEDEX_SITE}
 -dir           ${PHEDEX_STATE}
 -db            ${DBPARAM}:${TMDB}
 -wait          $(expr 60 + $RANDOM % 60)

#### Download agents
### AGENT LABEL=ral-download PROGRAM=Toolkit/Transfer/FileDownload DEFAULT=off ENVIRON=RAL
 -node ${PHEDEX_SITE}_Buffer
 -db ${DBPARAM}:${TMDB}
 -backend Globus
 -command /bin/true
 -pfndest ${PHEDEX_CUSTOM}/Transfer_URLmapper.sh,node=${PHEDEX_SITE}
 -publish ${PHEDEX_CUSTOM}/Publish.pl,${PHEDEX_CATALOGUE},${PHEDEX_SITE}
 -timeout 600
 -wanted 200G
 -jobs 1
 -wait $(expr 10 + $RANDOM % 10)
 
EOF

And another site:

cat > CNAF.config << "EOF"
### ENVIRON CNAF
PHEDEX_SITE=T1_CNAF;
PHEDEX_LOGS=$BASE/$PHEDEX_SITE/logs;
PHEDEX_STATE=$BASE/$PHEDEX_SITE/incoming;

#### Export-side agents
### AGENT LABEL=cnaf-exp-pfn PROGRAM=Toolkit/Transfer/FilePFNExport ENVIRON=CNAF
 -node ${PHEDEX_SITE}_Buffer
 -ignore ${PHEDEX_SITE}_Buffer
 -db ${DBPARAM}:${TMDB}
 -pfnquery ${PHEDEX_CUSTOM}/PFNLookup.sh,-u,${PHEDEX_CATALOGUE},-n,castor
 -wait $(expr 10 + $RANDOM % 10)

### AGENT LABEL=cnaf-exp-disk PROGRAM=Toolkit/Transfer/FileDiskExport ENVIRON=CNAF
 -db ${DBPARAM}:${TMDB}
 -node ${PHEDEX_SITE}_Buffer
 -wait $(expr 50 + $RANDOM % 20)

### AGENT LABEL=cnaf-exp-upload PROGRAM=Toolkit/Transfer/FileMSSPublish ENVIRON=CNAF
 -node ${PHEDEX_SITE}_Buffer
 -mssnode ${PHEDEX_SITE}_MSS
 -db ${DBPARAM}:${TMDB}
 -wait $(expr 50 + $RANDOM "%" 20)


#### Download agents
### AGENT LABEL=cnaf-download PROGRAM=Toolkit/Transfer/FileDownload ENVIRON=CNAF
 -node ${PHEDEX_SITE}_Buffer
 -ignore ${PHEDEX_SITE}_MSS
 -db ${DBPARAM}:${TMDB}
 -backend Globus
 -command /bin/true
 -pfndest ${PHEDEX_CUSTOM}/Transfer_URLmapper.sh,node=${PHEDEX_SITE}
 -publish ${PHEDEX_CUSTOM}/Publish.pl,${PHEDEX_CATALOGUE},${PHEDEX_SITE}
 -timeout 600
 -wanted 1T
 -jobs 3
 -wait $(expr 10 + $RANDOM % 10)

# fake migration to tape (inverse MSSPublish)
### AGENT LABEL=cnaf-exp-migrate PROGRAM=Toolkit/Transfer/FileMSSPublish ENVIRON=CNAF
 -node ${PHEDEX_SITE}_MSS
 -mssnode ${PHEDEX_SITE}_Buffer
 -db ${DBPARAM}:${TMDB}
 -wait $(expr 50 + $RANDOM "%" 20)

### AGENT LABEL=cnaf-exp-clean PROGRAM=Toolkit/Transfer/FileFakeCleaner ENVIRON=CNAF
 -nodes ${PHEDEX_SITE}_Buffer
 -db ${DBPARAM}:${TMDB}
 -backoff 36000
 -wait $(expr 4 \* 3600 + $RANDOM "%" 1800)


#### Management agents
### AGENT LABEL=cnaf-mgmt-rtab PROGRAM=Toolkit/Infrastructure/NodeRouter ENVIRON=CNAF
 -nodes ${PHEDEX_SITE}_Buffer,${PHEDEX_SITE}_MSS
 -db ${DBPARAM}:${TMDB}
 -timeout 240
 -wait $(expr 50 + $RANDOM % 20)

### AGENT LABEL=cnaf-mgmt-router PROGRAM=Toolkit/Infrastructure/FileRouter ENVIRON=CNAF
 -node ${PHEDEX_SITE}_Buffer
 -nodes ${PHEDEX_SITE}_Buffer,${PHEDEX_SITE}_MSS
 -db ${DBPARAM}:${TMDB}
 -wait $(expr 50 + $RANDOM % 20)

#### Start monitoring
### AGENT LABEL=cnaf-info-ds PROGRAM=Toolkit/Monitoring/InfoDropStatus ENVIRON=CNAF
 -site ${PHEDEX_SITE}
 -dir ${PHEDEX_STATE}
 -db ${DBPARAM}:${TMDB}
 
EOF

To start and stop everything you can use:

Master -config Common.config,CNAF.config,RAL.config,CERN.config start all
Master -config Common.config,RAL.config,CERN.config,CNAF.config stop all 

There were small problems throughout this stage, mostly to do with using
up to date versions of agents and misconfigurations. This will likely be
an iterative process, although ideally all major bugs in the new version
will be thrashed out before you try this (they were in this case).

After running the test agents I dropped the new tables, recreated the
schema and redid the migration. This was an iterative process as well,
involving retagging the head of CVS and starting from there.

    PROBLEM: Database disconnected on some of the downloads
    2005-06-07 19:23:04: FileDownload[22145]: alert: failed to mark 
    22556684-762A-D811-800F-00D0B7B875F5 in transfer: 
    DBD::Oracle::db prepare failed: ERROR Database disconnected 
    [for Statement "update t_transfer_state set to_state = 2, 
    to_timestamp = :now where guid = :guid and to_node = :to_node"] 
    at /data/tim/MigrationNodes/PHEDEX/Toolkit/Transfer/../../
    Toolkit/Common/UtilsDB.pm line 182.
    
    SOLUTION: Simple overload of DB? Doesn't always appear.

4. Drop old Testbed schema
--------------------------


5. Migrate Dev
--------------

To recap the process then, without the transfer of data. The following complete migration
sequence is a mixture of what happens above.

5.0 Preparation
5.1 Save existing schema
5.2 Load new schema
5.3 Map old data onto new schema
5.4 Issues

I'll start from a blank base install, on cmslcgco04, using the tagged version
PHEDEX_V2_2_20050607.

5.0 Preparation
---------------

# export DEVUSER=XXX
# export DEVPASS=XXX

export BASE=/data/tim/MigrationNodes
export CVSROOT=:pserver:anonymous@cmscvs.cern.ch:/cvs_server/repositories/PHEDEX
cvs login
mkdir -p $BASE
cd $BASE
cvs co -r PHEDEX_V2_2_20050607 PHEDEX
mkdir $BASE/Tools

# Copied the DBParam file to $BASE
# Copied the Oracle Instant Client zips to $BASE/Tools

export PATH=$PATH:$BASE/PHEDEX/Deployment:$BASE/PHEDEX/Utilities 
cd Tools
VO_CMS_SW_DIR=/afs/cern.ch/sw InstallPOOL -arch SLC3 -cms $BASE/Tools
. ./poolenv.sh
InstallOracleClient $BASE/Tools $BASE/Tools $BASE/PHEDEX/Schema
. ./oraenv.sh
InstallPerlModules $BASE/Tools
. ./perlenv.sh


5.1 Save existing schema
------------------------

cd $BASE/PHEDEX/Schema
./OracleSave.sh $DEVUSER/$DEVPASS@devdb
sqlplus $DEVUSER/$DEVPASS@devdb
    > alter trigger new_transfer_state rename to x_new_transfer_state;
    > alter trigger update_transfer_state rename to x_update_transfer_state;
    > commit;

5.2 Load new schema
-------------------

perl -p -i -e 's/([ ])CMS_TRANSFERMGMT_INDX01/${1}INDX01/g' *.sql
sqlplus ${DEVUSER}/${DEVPASS}@devdb < OracleInit.sql

5.3 Map old data onto new schema
--------------------------------

sqlplus $DEVUSER/$DEVPASS@devdb
    > insert into t_node
        (select name from xt_node);
    > insert into t_file 
        (select timestamp,guid,node,inblock,insubblock,
            lfn,filetype,filesize,checksum
        from xt_file);
    > insert into t_file_attributes
        (select guid,attribute,value from xt_file_attributes);
    > insert into t_node_import
        (select node,protocol,priority from xt_node_import);
    > insert into t_node_export
        (select node,protocol from xt_node_export);
    > insert into t_node_neighbour 
        (select from_node,to_node,hops
            from xt_routing
            where to_node = gateway);
    > insert into t_replica_state
        (select timestamp,guid,node,state,state_timestamp 
            from xt_replica_state);
    > alter trigger new_transfer_state disable;
    > alter trigger update_transfer_state disable;
    > insert into t_transfer_state
        (select timestamp,guid,to_node,to_state,to_timestamp,
                from_node,from_state,from_timestamp,from_pfn
            from xt_transfer_state);
    > alter trigger new_transfer_state enable;
    > alter trigger update_transfer_state enable;
    > insert into t_transfer_completed
        (select timestamp,guid,to_node,to_state,to_timestamp,
                from_node,from_state,from_timestamp
            from xt_transfer_completed);    
    > insert into t_transfer_history
        (select timestamp,guid,to_node,to_old_state,to_new_state,
                from_node,from_old_state,from_new_state
            from xt_transfer_history);
    > insert into t_transfer_summary
        (select timestamp,guid,from_node,to_node,assigned,wanted1st,wanted,
                exported,started,completed,cleared,errors,error_total,error_began
            from xt_transfer_summary);
    > insert into t_subscription
        (select owner,dataset,destination from xt_subscription);
    > insert into t_block 
        (select 0, name, owner, dataset, 0, 0, 1 
            from xt_block);
    > update t_block 
        set (timestamp, files, bytes) = 
            (select min(timestamp), count(guid), sum(filesize) 
                from t_file 
                where inblock = name);

5.4 Issues
----------

    PROBLEM: [5.3] Thought I'd nuked t_transfer_history on Dev. When I
    migrated, it inserted 14 rows. Checking, there were 14 rows in
    xt_transfer_history. This worries me. It probably wasn't created
    by OracleSave.sh. Could something have run against Dev while
    testing?


6. Create roles, synonyms etc for Dev
-------------------------------------

6.0 Preparation
6.1 Create role tables in TMDB
6.2 Create CERN role
6.3 Create synonyms

    FIXME: At first this was done on cmslcgco04. Now Lassi is running
    Dev agents on lxgate10, I've swapped there.

6.0 Preparation
---------------

# export DEVUSER=XXX
# export DEVPASS=XXX
export BASE=/data/DevNodes
cd $BASE/tools
. ./poolenv.sh
. ./oraenv.sh
. ./perlenv.sh

6.1 Create role tables in TMDB
------------------------------

Working from HEAD, some modifications to Schema

cd $BASE/PHEDEX/Schema
cvs update -A
perl -p -i -e 's/([ ])CMS_TRANSFERMGMT_INDX01/${1}INDX01/g' *.sql
sqlplus $DEVUSER/$DEVPASS@devdb @OracleCoreAuth.sql

6.2 Create CERN role
--------------------

Creating a CERN role, for Lassi

cd $BASE/PHEDEX/Utilities
cvs update -A

Had to modify WordMunger, as dictionary is slightly different
retrieving revision 1.2
diff -r1.2 WordMunger
1c1
< perl -e 'open d,"/usr/share/dict/web2";@w=<d>;close d;chomp @w;($f.=$w[rand $#w])until(length($f)>10);$f=substr($f,0,10);$f=~s/(\w)/a($1)/ge;while($c<2){$s=rand(length($f));$f=substr($f,0,$s).b().substr($f,$s,length($f));$c++;}print "$f\n"; sub a { rand(100) < 20 ? "\u$_[0]" : "\l$_[0]" } sub b { @o=("_","1","2","3","4","5","6","7","8","9","0"); return $o[rand($#o)];}'
---
> perl -e 'open d,"/usr/share/dict/linux.words";@w=<d>;close d;chomp @w;($f.=$w[rand $#w])until(length($f)>10);$f=substr($f,0,10);$f=~s/(\w)/a($1)/ge;while($c<2){$s=rand(length($f));$f=substr($f,0,$s).b().substr($f,$s,length($f));$c++;}print "$f\n"; sub a { rand(100) < 20 ? "\u$_[0]" : "\l$_[0]" } sub b { @o=("_","1","2","3","4","5","6","7","8","9","0"); return $o[rand($#o)];}'

mkdir -p $BASE/Keys
cd $BASE/Keys

# Copy public keys from lxgate10:/data/V2Nodes/Keys to here

cd $BASE/PHEDEX/Schema
# export DEVREADER=XXX
# export DEVWRITER=XXX
# export DEVWRITERPASS=XXX
# export DEVREADERPASS=XXX
ROLE_NAME=site_cern
ROLE_PASS=`../Utilities/WordMunger`
./OracleNewRole.sh $DEVUSER/$DEVPASS@devdb $ROLE_NAME $ROLE_PASS 
echo "insert into t_authorisation values \
    (`date +%s`,'$ROLE_NAME','lassi.tuura@cern.ch', \
    '/C=CH/O=CERN/OU=GRID/CN=Lassi Tuura 3370');"  \
    | sqlplus $DEVUSER/$DEVPASS@devdb


# FIXME: variables not substituted in!

cd $BASE/Keys
cat > site_cern-Dev << "EOF"
AuthDBPassword          $DEVWRITERPASS
AuthRole                $ROLE_NAME
AuthRolePassword        $ROLE_PASS
EOF

openssl smime -encrypt  -in site_cern-Dev -out site_cern-Dev.Jens ./jens.rehn@cern.ch
openssl smime -encrypt  -in site_cern-Dev -out site_cern-Dev.Lassi ./lassi.tuura@cern.ch   

    PROBLEM: In doing this for Daniele found DN too long for field
    
    SOLUTION: Increase size of field in devdb and OracleCoreAuth.sql.

6.3 Create synonyms
-------------------

./OracleSyns.sh $DEVUSER $DEVUSER/$DEVPASS@devdb \
    $DEVWRITER/$DEVWRITERPASS@devdb
./OracleSyns.sh $DEVUSER $DEVUSER/$DEVPASS@devdb \
    $DEVREADER/$DEVREADERPASS@devdb
    
    
9 Migrate Prod
--------------

9.0 Preparation
9.1 Drop existing connections, change passwords
9.2 Save existing schema
9.3 Load new schema
9.4 Map old data onto new schema
9.6 Roles and privileges


9.0 Preparation
---------------

# export ADMIN=XXX
# export ADMINPASS=XXX
# export WRITER=XXX
# export WRITERPASS=XXX
# export READER=XXX
# export READERPASS=XXX

export BASE=/data/MigrationNodes 
mkdir -p $BASE
cd $BASE

mkdir $BASE/tools
export CVSROOT=:pserver:anonymous@cmscvs.cern.ch:/cvs_server/repositories/PHEDEX
cvs login
cvs co PHEDEX

export PATH=$PATH:$BASE/PHEDEX/Deployment:$BASE/PHEDEX/Utilities 
cd tools
VO_CMS_SW_DIR=/afs/cern.ch/sw InstallPOOL -arch SLC3 -cms $BASE/tools
. ./poolenv.sh
# Copy Oracle Instant Client zips to $BASE/tools
InstallOracleClient $BASE/tools/OIC-zips $BASE/tools $BASE/PHEDEX/Schema
. ./oraenv.sh
InstallPerlModules $BASE/Tools
. ./perlenv.sh
cd $BASE

# NOTE, on restart just need
export BASE=/data/MigrationNodes 
export PATH=$PATH:$BASE/PHEDEX/Deployment:$BASE/PHEDEX/Utilities 
cd $BASE/tools
. ./poolenv.sh
. ./oraenv.sh
. ./perlenv.sh
cd $BASE

9.1 Drop existing connections, change passwords
-----------------------------------------------

# Logged in as cms_transfermgmt* on the web-based oracle session manager,
# Killed all sessions

export ADMINPASS=`/data/MigrationNodes/PHEDEX/Utilities/WordMunger`
export WRITERPASS=`/data/MigrationNodes/PHEDEX/Utilities/WordMunger`
export READERPASS=`/data/MigrationNodes/PHEDEX/Utilities/WordMunger`

# Created file /data/V2Nodes/Keys/Details/Prod containing
# content:
# cms_transfermgmt                XXX
# cms_transfermgmt_writer         XXX
# cms_transfermgmt_reader         XXX
# for reference...

9.2 Save existing schema
------------------------

cd $BASE/PHEDEX/Schema
sqlplus $ADMIN/$ADMINPASS@cms
    > alter index IX_TRANSFER_STATE_FROMTO_STATE rename to XIX_FROMTO_STATE;
    > commit;
./OracleSave.sh $ADMIN/$ADMINPASS@cms

sqlplus $ADMIN/$ADMINPASS@devdb
    > alter trigger new_transfer_state rename to x_new_transfer_state;
    > alter trigger update_transfer_state rename to x_update_transfer_state;
    > commit;

9.3 Load new schema
-------------------

cd $BASE/PHEDEX/Schema

# Modify the SQL to use cms tablespaces!
perl -p -i -e 's/([ ])INDX01/${1}CMS_TRANSFERMGMT_INDX01/g' *.sql

sqlplus $ADMIN/$ADMINPASS@cms < OracleInit.sql

9.4 Map old data onto new schema
--------------------------------

cat > migrate.sql << "EOF"
insert into t_node
        (select name from xt_node);
commit;
insert into t_file 
        (select timestamp,guid,node,inblock,insubblock,
            lfn,filetype,filesize,checksum
        from xt_file);
commit;
insert into t_file_attributes
        (select guid,attribute,value from xt_file_attributes);
commit;
insert into t_node_import
        (select node,protocol,priority from xt_node_import);
commit;
insert into t_node_export
        (select node,protocol from xt_node_export);
commit;
insert into t_node_neighbour 
        (select from_node,to_node,hops
            from xt_routing
            where to_node = gateway);
commit;
insert into t_replica_state
        (select timestamp,guid,node,state,state_timestamp 
            from xt_replica_state);
commit;
alter trigger new_transfer_state disable;
alter trigger update_transfer_state disable;
insert into t_transfer_state
        (select timestamp,guid,to_node,to_state,to_timestamp,
                from_node,from_state,from_timestamp,from_pfn
            from xt_transfer_state);
alter trigger new_transfer_state enable;
alter trigger update_transfer_state enable;
commit;
insert into t_transfer_completed
        (select timestamp,guid,to_node,to_state,to_timestamp,
                from_node,from_state,from_timestamp
            from xt_transfer_completed);    
commit;
insert into t_transfer_history
        (select timestamp,guid,to_node,to_old_state,to_new_state,
                from_node,from_old_state,from_new_state
            from xt_transfer_history);
commit;
insert into t_transfer_summary
        (select timestamp,guid,from_node,to_node,assigned,wanted1st,wanted,
                exported,started,completed,cleared,errors,error_total,error_began
            from xt_transfer_summary);
commit;
insert into t_subscription
        (select owner,dataset,destination from xt_subscription);
commit;
insert into t_block 
        (select 0, name, owner, dataset, 0, 0, 1 
            from xt_block);
commit;
update t_block 
        set (timestamp, files, bytes) = 
            (select min(timestamp), count(guid), sum(filesize) 
                from t_file 
                where inblock = name);
commit;
EOF

sqlplus $ADMIN/$ADMINPASS@cms @migrate.sql

    PROBLEM: No space!
    SOLUTION: drop all t_* tables; recreate schema; get some temp space AND
    archive t_transfer_history.


9.6 Roles and privileges
------------------------

# Creating roles and passwords in one go using
ROLE_NAME=site_x
ROLE_PASS=`$BASE/PHEDEX/Utilities/WordMunger`
./OracleNewRole.sh $ADMIN/$ADMINPASS@cms $ROLE_NAME $ROLE_PASS 

# Building a file $BASE/PHEDEX/Schema/roles.sh
# echo "insert into t_authorisation values (`date +%s`,'site_cern','lassi.tuura@cern.ch','/C=CH/O=CERN/OU=GRID/CN=Lassi Tuura 3370');" | sqlplus $ADMIN/$ADMINPASS@cms;
# echo "insert into t_authorisation values (`date +%s`,'site_in2p3','igor.semeniouk@poly.in2p3.fr','/O=GRID-FR/C=FR/O=CNRS/OU=LLR/CN=Igor Semeniouk');" | sqlplus $ADMIN/$ADMINPASS@cms;
...
echo "insert into t_authorisation values (`date +%s`,'site_','','');" | sqlplus $ADMIN/$ADMINPASS@cms;

# creating/modigfying /data/V2Nodes/Keys/Details/$ROLE_NAME files as well
# encrypting and sending to recipients

When migration complete, need to
. $BASE/PHEDEX/Schema/roles.sql

$BASE/PHEDEX/Schema/OraclePrivs.sh $ADMIN/$ADMINPASS@cms $READER $WRITER
$BASE/PHEDEX/Schema/OracleSyns.sh $ADMIN $ADMIN/$ADMINPASS@cms $WRITER/$WRITERPASS@cms
$BASE/PHEDEX/Schema/OracleSyns.sh $ADMIN $ADMIN/$ADMINPASS@cms $READER/$READERPASS@cms

9.7 Stats
---------

$BASE/PHEDEX/Schema/OracleStatsEnable.sh $ADMIN/$ADMINPASS@cms
