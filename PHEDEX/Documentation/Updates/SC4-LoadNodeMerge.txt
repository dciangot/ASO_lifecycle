/*
		     SC4 Deletion of "Load" nodes
  From late April 2006 to January 2007 the SC4 instance was used for
  load testing.  At this time sites ran special "_Load" nodes in order
  to clearly separate the traffic.  This was uneccessary after PhEDEx
  2.3, but the _Load nodes remained in the SC4 instance.  SC4 was not
  used from January 2007 to July 2007, but in July it was decided it
  would be ressurected to be the "Debug" instance for commissioning
  links.  To reduce confusion, we need to delete the old _Load nodes
  -- but we wish to keep the statistics of the 2006 activities for
  future reference.  Hence this migration merges the statistics of the
  _Load nodes with their associated _Buffer node, so that the _Load
  nodes may be deleted.

  Note that we do *not* aim for a perfectly accurate merging in the
  case of non-cumulnative data.  The fact is that the _Load nodes did
  a lot more transferring than the _Buffer nodes in the SC4 instance,
  so in any cases where weighted averaging would be required we simply
  drop the _Buffer data.  If someone is sufficiently motivated in the
  future, it is possible to recalculate these columns (transfer rates
  and transfer latencies) using other statistics which are merged in a
  perfectly accurate way.  */

# Set authentication for DB access
export PHEDEX_DB='SITECONF/CERN/PhEDEx/DBParam:SC4/Admin';

# Clear old xt_ tables
sqlplus -S $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB}) @PHEDEX/Schema/OracleResetOld.sql </dev/null

# Duplicate current tables
sqlplus -S $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB}) @PHEDEX/Schema/OracleDuplicate.sql </dev/null

# Connect to DB using SQLPlus
sqlplus $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB})

-- Create a node_map table
create table node_map as
select n1.id old, n2.id new 
  from (select id, name, replace(name, '_Load', '_Buffer') new_name 
          from t_adm_node) n1 
  join t_adm_node n2 on n1.new_name = n2.name
where n1.id != n2.id;

-- Some nodes do not follow the naming convention, add these by hand
T2_Belgium_IIHE_Load --> T2_Belgium_IIHE
T2_GRIF_Load         --> T2_GRIF_LLR
T2_Spain_Load        --> T2_Spain_CIEMAT
T2_Spain_IFCA_Load   --> T2_Spain_IFCA

-- merge history
-- We need to do the from_node and to_node separately to avoid duplicate updates
-- which trigger an ORA-00001.  We do this by setting a variable 'do_from' to 0 or 1
-- write the following SQL to the files 'history_sums.sql' and
-- 'merge.sql' and execute the following:
--   sqlplus> spool history_before;
--   sqlplus> @history_sums;
--   sqlplus> spool off;
--   sqlplus> var do_from number;
--   sqlplus> exec :do_from := 1;
--   sqlplus> @merge
--   sqlplus> exec :do_from := 0;
--   sqlplus> @merge
--   sqlplus> commit;
--   sqlplus> spool history_after;
--   sqlplus> @history_sums;
--   sqlplus> spool off;

--		  ===== BEGIN history_sums.sql =====
set linesize 120;
select
  sum(nvl(h.avail_files,0)) af, sum(nvl(h.avail_bytes,0)) ab,
  sum(nvl(h.done_files,0)) df, sum(nvl(h.done_bytes,0)) db,
  sum(nvl(h.try_files,0)) tf, sum(nvl(h.try_bytes,0)) tb,
  sum(nvl(h.fail_files,0)) ff, sum(nvl(h.fail_bytes,0)) fb,
  sum(nvl(h.expire_files,0)) ef, sum(nvl(h.expire_bytes,0)) eb
from t_history_link_events h;

select
  sum(nvl(h.pend_files,0)) pf, sum(nvl(h.pend_bytes,0)) pb,
  sum(nvl(h.wait_files,0)) wf, sum(nvl(h.wait_bytes,0)) wb,
  sum(nvl(h.cool_files,0)) cf, sum(nvl(h.cool_bytes,0)) cb,
  sum(nvl(h.ready_files,0)) rf, sum(nvl(h.ready_bytes,0)) rb,
  sum(nvl(h.xfer_files,0)) xf, sum(nvl(h.xfer_bytes,0)) xb,
  sum(nvl(h.confirm_files,0)) cff, sum(nvl(h.confirm_bytes,0)) cfb, sum(nvl(h.confirm_weight,0)) cfw,
  sum(nvl(h.param_rate,0)) pmf, sum(nvl(h.param_latency,0)) pmb
from t_history_link_stats h;

select
  sum(nvl(h.dest_files,0)) df, sum(nvl(h.dest_bytes,0)) db,
  sum(nvl(h.src_files,0)) sf, sum(nvl(h.src_bytes,0)) sb,
  sum(nvl(h.node_files,0)) nf, sum(nvl(h.node_bytes,0)) nb,
  sum(nvl(h.request_files,0)) rf, sum(nvl(h.request_bytes,0)) rb,
  sum(nvl(h.idle_files,0)) if, sum(nvl(h.idle_bytes,0)) ib
from t_history_dest h;
--		  ===== END history_sums.sql =====

--		  ===== BEGIN merge.sql =====
set timing on;
merge /*+ append */ into t_history_link_events h
using 
(
select decode(:do_from, 1, fn.new, le.from_node) new_from,
       decode(:do_from, 0, tn.new, le.to_node) new_to,
       le.*
  from t_history_link_events le
       left join node_map fn on fn.old = le.from_node
       left join node_map tn on tn.old = le.to_node
 where (:do_from = 1 and fn.new is not null)
    or (:do_from = 0 and tn.new is not null)
) v
on (h.timebin = v.timebin
    and h.priority = v.priority
    and h.from_node = v.new_from
    and h.to_node = v.new_to)
when matched then
  update set
    h.avail_files = (nvl(h.avail_files,0) + nvl(v.avail_files,0)),
    h.avail_bytes = (nvl(h.avail_bytes,0) + nvl(v.avail_bytes,0)),
    h.done_files = (nvl(h.done_files,0) + nvl(v.done_files,0)),
    h.done_bytes = (nvl(h.done_bytes,0) + nvl(v.done_bytes,0)),
    h.try_files = (nvl(h.try_files,0) + nvl(v.try_files,0)),
    h.try_bytes = (nvl(h.try_bytes,0) + nvl(v.try_bytes,0)),
    h.fail_files = (nvl(h.fail_files,0) + nvl(v.fail_files,0)),
    h.fail_bytes = (nvl(h.fail_bytes,0) + nvl(v.fail_bytes,0)),
    h.expire_files = (nvl(h.expire_files,0) + nvl(v.expire_files,0)),
    h.expire_bytes = (nvl(h.expire_bytes,0) + nvl(v.expire_bytes,0))
when not matched then
  insert (h.timebin, h.timewidth,
          h.from_node, h.to_node, h.priority,
          h.avail_files, h.avail_bytes,
          h.done_files, h.done_bytes,
          h.try_files, h.try_bytes,
          h.fail_files, h.fail_bytes,
          h.expire_files, h.expire_bytes)
  values (v.timebin, v.timewidth,
          v.new_from, v.new_to, v.priority,
          v.avail_files, v.avail_bytes,
          v.done_files, v.done_bytes,
          v.try_files, v.try_bytes,
          v.fail_files, v.fail_bytes,
          v.expire_files, v.expire_bytes)
;

delete from t_history_link_events
 where (:do_from = 1 and from_node in (select old from node_map))
    or (:do_from = 0 and to_node in (select old from node_map));

-- merge link stats history
merge /*+ append */ into t_history_link_stats h
using 
(
select decode(:do_from, 1, fn.new, ls.from_node) new_from,
       decode(:do_from, 0, tn.new, ls.to_node) new_to,
       ls.*
  from t_history_link_stats ls
       left join node_map fn on fn.old = ls.from_node
       left join node_map tn on tn.old = ls.to_node
 where (:do_from = 1 and fn.new is not null)
    or (:do_from = 0 and tn.new is not null) 
) v
on (h.timebin = v.timebin
    and h.priority = v.priority
    and h.from_node = v.new_from
    and h.to_node = v.new_to)
when matched then
  update set
    h.pend_files = (nvl(h.pend_files,0) + nvl(v.pend_files,0)),
    h.pend_bytes = (nvl(h.pend_bytes,0) + nvl(v.pend_bytes,0)),
    h.wait_files = (nvl(h.wait_files,0) + nvl(v.wait_files,0)),
    h.wait_bytes = (nvl(h.wait_bytes,0) + nvl(v.wait_bytes,0)),
    h.cool_files = (nvl(h.cool_files,0) + nvl(v.cool_files,0)),
    h.cool_bytes = (nvl(h.cool_bytes,0) + nvl(v.cool_bytes,0)),
    h.ready_files = (nvl(h.ready_files,0) + nvl(v.ready_files,0)),
    h.ready_bytes = (nvl(h.ready_bytes,0) + nvl(v.ready_bytes,0)),
    h.xfer_files = (nvl(h.xfer_files,0) + nvl(v.xfer_files,0)),
    h.xfer_bytes = (nvl(h.xfer_bytes,0) + nvl(v.xfer_bytes,0)),
    h.confirm_files = (nvl(h.confirm_files,0) + nvl(v.confirm_files,0)),
    h.confirm_bytes = (nvl(h.confirm_bytes,0) + nvl(v.confirm_bytes,0)),
    h.confirm_weight = v.confirm_weight, -- Obsolete column, prefer _Load data
    -- Approximations:  n_bytes _Load data >> n_bytes _Buffer data
    h.param_rate = v.param_rate,  
    h.param_latency = v.param_latency
when not matched then
  insert (h.timebin, h.timewidth,
          h.from_node, h.to_node, h.priority,
          h.pend_files, h.pend_bytes,
          h.wait_files, h.wait_bytes,
          h.cool_files, h.cool_bytes,
          h.ready_files, h.ready_bytes,
          h.xfer_files, h.xfer_bytes,
          h.confirm_files, h.confirm_bytes, h.confirm_weight,
          h.param_rate, h.param_latency)
  values (v.timebin, v.timewidth,
          v.new_from, v.new_to, v.priority,
          v.pend_files, v.pend_bytes,
          v.wait_files, v.wait_bytes,
          v.cool_files, v.cool_bytes,
          v.ready_files, v.ready_bytes,
          v.xfer_files, v.xfer_bytes,
          v.confirm_files, v.confirm_bytes, v.confirm_weight,
          v.param_rate, v.param_latency)
;

delete from t_history_link_stats
 where (:do_from = 1 and from_node in (select old from node_map))
    or (:do_from = 0 and to_node in (select old from node_map));

-- merge block dest history
merge /*+ append */ into t_history_dest h
using 
(
select n.new new_node, d.*
from t_history_dest d
join node_map n on n.old = d.node and :do_from = 0
) v
on (h.timebin = v.timebin
    and h.timewidth = v.timewidth
    and h.node = v.new_node)
when matched then
  update set
    h.dest_files = (nvl(h.dest_files,0) + nvl(v.dest_files,0)),
    h.dest_bytes = (nvl(h.dest_bytes,0) + nvl(v.dest_bytes,0)),
    h.src_files = (nvl(h.src_files,0) + nvl(v.src_files,0)),
    h.src_bytes = (nvl(h.src_bytes,0) + nvl(v.src_bytes,0)),
    h.node_files = (nvl(h.node_files,0) + nvl(v.node_files,0)),
    h.node_bytes = (nvl(h.node_bytes,0) + nvl(v.node_bytes,0)),
    h.request_files = (nvl(h.request_files,0) + nvl(v.request_files,0)),
    h.request_bytes = (nvl(h.request_bytes,0) + nvl(v.request_bytes,0)),
    h.idle_files = (nvl(h.idle_files,0) + nvl(v.idle_files,0)),
    h.idle_bytes = (nvl(h.idle_bytes,0) + nvl(v.idle_bytes,0))
when not matched then
  insert (h.timebin, h.timewidth,
          h.node,
          h.dest_files, h.dest_bytes,
          h.src_files, h.src_bytes,
          h.node_files, h.node_bytes,
          h.request_files, h.request_bytes,
          h.idle_files, h.idle_bytes)
  values (v.timebin, v.timewidth,
          v.new_node,
          v.dest_files, v.dest_bytes,
          v.src_files, v.src_bytes,
          v.node_files, v.node_bytes,
          v.request_files, v.request_bytes,
          v.idle_files, v.idle_bytes)
;

delete from t_history_dest where :do_from = 0 and node in (select old from node_map);
--		  ===== END merge.sql =====


# Compare history (back to shell)
diff history_before history_after;

		       ##### Differences #####
< 3.0803E+17  167936910 3.6015E+12 4.2351E+11

---
> 3.0803E+17  165633026 3.5949E+12 4.0105E+11
		       #######################
# (As expected the differences are only in the columns we didn't
#  attempt to accurately merge)


-- delete the _Load nodes (back to sqlplus)
delete from t_adm_node where id in (select old from node_map);

commit;

-- remove the node map table
drop table node_map;

exit;


# Connect to Prod instance
export PHEDEX_DB='SITECONF/CERN/PhEDEx/DBParam:Prod/Admin';
sqlplus $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB});

-- Grant access to SC4 instance
grant select on t_adm_node to cms_transfermgmt_sc;

-- Connect back to SC4 instance
exit;
export PHEDEX_DB='SITECONF/CERN/PhEDEx/DBParam:SC4/Admin';
sqlplus $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB});

-- Merge nodes from Prod
merge into cms_transfermgmt_sc.t_adm_node s
using
cms_transfermgmt.t_adm_node p
on (s.name = p.name)
when matched then
update set
  s.kind = p.kind,
  s.technology = p.technology,
  s.se_name = p.se_name,
  s.capacity = p.capacity,
  s.bandwidth_cap = p.bandwidth_cap
when not matched then insert
(s.id, s.name, s.kind, s.technology,
 s.se_name, s.capacity, s.bandwidth_cap)
values
(seq_adm_node.nextval, p.name, p.kind, p.technology,
 p.se_name, p.capacity, p.bandwidth_cap)
;

-- View differences
select p.id prod_id, p.name prod_name, s.id sc_id, s.name sc_name
  from cms_transfermgmt.t_adm_node p
       full outer join t_adm_node s
         on s.name = p.name
 where p.name is null or s.name is null;

commit;

-- Update node-partitioned tables
begin
  for o in (select table_name, partition_name from user_tab_partitions
             where table_name like 'T_XFER_%' and partition_name not like '%_DUMMY') loop
    execute immediate 'alter table ' || o.table_name || ' drop partition ' || o.partition_name;
  end loop;
end;
/

begin
  for o in (select id, name from t_adm_node) loop
    execute immediate 'alter table t_xfer_replica add partition node_' || lower(o.name) || ' values (' || o.id || ')';
    execute immediate 'alter table t_xfer_request add partition dest_' || lower(o.name) || ' values (' || o.id || ')';
    execute immediate 'alter table t_xfer_task    add partition to_'   || lower(o.name) || ' values (' || o.id || ')';
  end loop;
end;
/


-- Revoke permissions
exit;
export PHEDEX_DB='SITECONF/CERN/PhEDEx/DBParam:Prod/Admin';
sqlplus $(PHEDEX/Utilities/OracleConnectId  -db ${PHEDEX_DB});

-- Revoke access from SC4 instance
revoke select on t_adm_node from cms_transfermgmt_sc;

exit;

-- Update statistics
export PHEDEX_DB='SITECONF/CERN/PhEDEx/DBParam:SC4/Admin';
sqlplus -S $(PHEDEX/Utilities/OracleConnectId  -db PHEDEX_DB) @PHEDEX/Schema/OracleStatsUpdate.sql </dev/null
